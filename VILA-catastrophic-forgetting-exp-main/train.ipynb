{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5d3f402-4b0e-4a88-a3de-6fdb4cf0f70a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting setuptools==58.0.0\n",
      "  Downloading setuptools-58.0.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Downloading setuptools-58.0.0-py3-none-any.whl (816 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m816.3/816.3 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: setuptools\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 72.1.0\n",
      "    Uninstalling setuptools-72.1.0:\n",
      "      Successfully uninstalled setuptools-72.1.0\n",
      "Successfully installed setuptools-58.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install setuptools==58.0.0 # ÈôçÁ∫ßsetuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2583bf44-8574-475e-8efc-5586e3afc623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sh environment_setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd01cf06-23e8-41f2-8684-9de856c9fe1d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Âä†ËΩΩÂ∞ùËØï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe234c5-f57b-4a1f-b175-cb418749bada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ââ™ÊûùÂêésize‰∏çÂåπÈÖçÈóÆÈ¢òÔºö\n",
    "# 1. strict=False,‰∏çË°å\n",
    "# 2. ÈáçÊñ∞Âä†ËΩΩ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93b8f8c5-bb5c-443a-921d-56655a6b829e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d00dc38c44d4feda0f504369a282f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me something about large language models: what is happening at Google‚Äôs artificially intelligent language and context-based. How do you explain why these giant corporations are getting in on trend a little late? The company‚Äôs artificial intelligence programs are an interesting example of a technology that may be used to the benefit of the technology.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, LlamaForCausalLM\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "\n",
    "model_name = \"../outputs/vila-llm-pruned-mask\"\n",
    "\n",
    "# # ‰ΩøÁî®Ëá™ÂÆö‰πâÁöÑ Llama Ê®°Âûã\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"../outputs/vila-llm-pruned-mask\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    use_safetensors=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "# config = AutoConfig.from_pretrained(\"../outputs/VILA1.5-3b/llm\")\n",
    "# config.hidden_size = 2432  # Ê†πÊçÆÂâ™ÊûùÂêéÁöÑÊ®°ÂûãË∞ÉÊï¥hidden_size\n",
    "# config.num_attention_heads = 19  # Ê†πÊçÆÂâ™ÊûùÂêéÁöÑÊ®°ÂûãË∞ÉÊï¥hidden_size\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16,ignore_mismatched_sizes=True,config=config,\n",
    "#     use_safetensors=True).to(\"cuda\")\n",
    "\n",
    "# state_dict = load_file(\"../outputs/VILA1.5-3b/llm/model-00001-of-00002.safetensors\")\n",
    "# model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "# # Âä†ËΩΩÁ¨¨‰∫åÈÉ®ÂàÜÊùÉÈáç\n",
    "# state_dict_part2 = load_file(\"../outputs/VILA1.5-3b/llm/model-00002-of-00002.safetensors\")\n",
    "# model.load_state_dict(state_dict_part2, strict=False)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('../outputs/VILA1.5-3b/llm')\n",
    "input_text = \"Tell me something about large language models:\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids, max_length=100, num_return_sequences=1, do_sample=True)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f844e0f8-99d9-4eb6-97b3-07b75127fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÈúÄË¶Å‰øùËØÅÊï¥Èô§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc5d99a-09c5-4fa5-90a1-825d07779bef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, LlamaForCausalLM\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "class CustomLlamaForCausalLM(LlamaForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        print(config)\n",
    "        config.num_attention_heads = 16\n",
    "        config.head_dim = 128\n",
    "        super().__init__(config)\n",
    "        \n",
    "        for layer in self.model.layers:\n",
    "            layer.self_attn.q_proj = nn.Linear(2560, 2048)  # Ëá™ÂÆö‰πâÂ∞∫ÂØ∏\n",
    "            layer.self_attn.k_proj = nn.Linear(2560, 2048)   # Ëá™ÂÆö‰πâÂ∞∫ÂØ∏\n",
    "            layer.self_attn.v_proj = nn.Linear(2560, 2048)   # Ëá™ÂÆö‰πâÂ∞∫ÂØ∏\n",
    "            layer.self_attn.o_proj = nn.Linear(2048, 2560)  # Ëá™ÂÆö‰πâÂ∞∫ÂØ∏\n",
    "\n",
    "# # ‰ΩøÁî®Ëá™ÂÆö‰πâÁöÑ Llama Ê®°Âûã\n",
    "model = CustomLlamaForCausalLM.from_pretrained(\n",
    "    \"../outputs/VILA1.5-3b/llm\",\n",
    "    ignore_mismatched_sizes=True,\n",
    "    use_safetensors=True\n",
    ").to(\"cuda\")\n",
    "model.num_attention_heads = 16\n",
    "model.head_dim = 128\n",
    "\n",
    "model_name = '../outputs/VILA1.5-3b/llm'\n",
    "# config = AutoConfig.from_pretrained(\"../outputs/VILA1.5-3b/llm\")\n",
    "# config.hidden_size = 2432  # Ê†πÊçÆÂâ™ÊûùÂêéÁöÑÊ®°ÂûãË∞ÉÊï¥hidden_size\n",
    "# config.num_attention_heads = 19  # Ê†πÊçÆÂâ™ÊûùÂêéÁöÑÊ®°ÂûãË∞ÉÊï¥hidden_size\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16,ignore_mismatched_sizes=True,config=config,\n",
    "#     use_safetensors=True).to(\"cuda\")\n",
    "\n",
    "# state_dict = load_file(\"../outputs/VILA1.5-3b/llm/model-00001-of-00002.safetensors\")\n",
    "# model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "# # Âä†ËΩΩÁ¨¨‰∫åÈÉ®ÂàÜÊùÉÈáç\n",
    "# state_dict_part2 = load_file(\"../outputs/VILA1.5-3b/llm/model-00002-of-00002.safetensors\")\n",
    "# model.load_state_dict(state_dict_part2, strict=False)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "input_text = \"Tell me something about large language models:\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids, max_length=100, num_return_sequences=1, do_sample=True)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e378a801-cf0e-41f6-bcf7-3ee9dac4576d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252169f9-b70f-46b0-8138-0ac525952fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb8ab23-89eb-4019-aaea-b3693ce41d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94a2641-c169-4786-9277-2c90887abd85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from safetensors.torch import load_file\n",
    "import torch\n",
    "\n",
    "# 1. Âä†ËΩΩÂéüÂßãÈ¢ÑËÆ≠ÁªÉÊ®°Âûã\n",
    "original_model_path = \"../../../base_model/VILA1.5-3b/llm\"\n",
    "model = AutoModelForCausalLM.from_pretrained(original_model_path)\n",
    "\n",
    "# 2. Âä†ËΩΩÊñ∞ÁöÑ safetensors ÊùÉÈáçÊñá‰ª∂\n",
    "safetensor_path = \"../outputs/VILA1.5-3b/llm/model-00001-of-00002.safetensors\"\n",
    "new_weights = load_file(safetensor_path)\n",
    "\n",
    "# 3. ÊõøÊç¢ÊåáÂÆöÊ®°ÂùóÁöÑÊùÉÈáçÔºàÊØîÂ¶Ç self_attn.q_projÔºâ\n",
    "with torch.no_grad():  # ÈÅøÂÖçËá™Âä®Ê¢ØÂ∫¶Ë∑üË∏™\n",
    "    for name, param in model.named_parameters():\n",
    "        if name in new_weights:\n",
    "            print(f\"Replacing weights for {name}\")\n",
    "            model.named_parameters()[name] = new_weights\n",
    "\n",
    "# 4. Ê£ÄÊü•ÊõøÊç¢ÊòØÂê¶ÊàêÂäü\n",
    "for name, param in model.named_parameters():\n",
    "    if name in new_weights:\n",
    "        assert torch.allclose(param, new_weights[name]), f\"Weight replacement for {name} failed!\"\n",
    "\n",
    "# 5. ‰øùÂ≠òÊõøÊç¢ÂêéÁöÑÊ®°Âûã\n",
    "output_path = \"../path_to_save_new_model\"\n",
    "model.save_pretrained(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d8b022-246a-4c6f-98cb-84645fe676e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ÂêØÂä®ËÆ≠ÁªÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aac2327b-9bbf-4768-abee-ac9782f8d595",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install wandb\n",
    "import wandb\n",
    "wandb.login(key='8b0b2edc05fa3d04079967e01434d12b52f5312e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "842b6756-bd6e-41df-9baa-5f41d939cd9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\t\t\t  special_tokens_map.json\n",
      "generation_config.json\t\t  tokenizer_config.json\n",
      "model-00001-of-00002.safetensors  tokenizer.model\n",
      "model-00002-of-00002.safetensors  vila-llm-pruned\n",
      "model.safetensors.index.json\n"
     ]
    }
   ],
   "source": [
    "!ls /home/ec2-user/SageMaker/efs/anker_llm_compression/scripts/outputs/VILA1.5-3b/llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e86dd6-4268-4161-918b-7023ddb999b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ëé∑ÂèñÂΩìÂâçllama‰ª£Á†Å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d1ceeb01-f44f-4cca-b8b7-d64b9c926220",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py modeling_llama_origin.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcd2b68-42bf-4b1a-8dbf-24fb20b7045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Áî®‰øÆÊîπÂêéÁöÑ‰ª£Á†ÅË¶ÜÁõñÂéüÊúâ‰ª£Á†Å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03b3a89c-552e-4b06-8524-e542566b92d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp modeling_llama_modified.py /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572339ce-c0de-4f2a-bc50-3b09cf08db96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ê£ÄÊü•ÈÖçÁΩÆÊñá‰ª∂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed2254c-016f-49c4-8b39-3d0679d5ec65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat /home/ec2-user/SageMaker/efs/anker_llm_compression/scripts/outputs/VILA1.5-3b-normal/llm/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2bb9d5da-cbc8-40da-8028-61e05beb663d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp config_chenwei.json ../outputs/VILA1.5-3b-normal/llm/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b3fea9-2c53-4e75-85a6-62ddea033957",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat ../outputs/VILA1.5-3b-normal/llm/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e23918-f4fc-405a-9e0a-f9d0b864d7d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scripts/v1_5/release/8b/3_sft_anker_a100.sh: line 3: scontrol: command not found\n",
      "scripts/v1_5/release/8b/3_sft_anker_a100.sh: line 6: scontrol: command not found\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Did not find AutoResume SDK!\n",
      "Did not find AutoResume SDK!\n",
      "Did not find AutoResume SDK!\n",
      "Did not find AutoResume SDK!\n",
      "Did not find AutoResume SDK!\n",
      "Did not find AutoResume SDK!\n",
      "Did not find AutoResume SDK!\n",
      "Did not find AutoResume SDK!\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.90it/s]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.77it/s]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.68it/s]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.64it/s]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.64it/s]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.65it/s]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.64it/s]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00,  1.19it/s]\n",
      "WARNING:root:You are setting tunable parameters for the model. Previous args include 'freeze_backbone' and 'tune_mm_mlp_adapter' are deprecated.\n",
      " Notice: default value of tune_xxx is False, which means you would not tune this part.\n",
      "WARNING:root:You are setting tunable parameters for the model. Previous args include 'freeze_backbone' and 'tune_mm_mlp_adapter' are deprecated.\n",
      " Notice: default value of tune_xxx is False, which means you would not tune this part.\n",
      "WARNING:root:You are setting tunable parameters for the model. Previous args include 'freeze_backbone' and 'tune_mm_mlp_adapter' are deprecated.\n",
      " Notice: default value of tune_xxx is False, which means you would not tune this part.\n",
      "WARNING:root:You are setting tunable parameters for the model. Previous args include 'freeze_backbone' and 'tune_mm_mlp_adapter' are deprecated.\n",
      " Notice: default value of tune_xxx is False, which means you would not tune this part.\n",
      "WARNING:root:You are setting tunable parameters for the model. Previous args include 'freeze_backbone' and 'tune_mm_mlp_adapter' are deprecated.\n",
      " Notice: default value of tune_xxx is False, which means you would not tune this part.\n",
      "WARNING:root:You are setting tunable parameters for the model. Previous args include 'freeze_backbone' and 'tune_mm_mlp_adapter' are deprecated.\n",
      " Notice: default value of tune_xxx is False, which means you would not tune this part.\n",
      "WARNING:root:You are setting tunable parameters for the model. Previous args include 'freeze_backbone' and 'tune_mm_mlp_adapter' are deprecated.\n",
      " Notice: default value of tune_xxx is False, which means you would not tune this part.\n",
      "WARNING:root:You are setting tunable parameters for the model. Previous args include 'freeze_backbone' and 'tune_mm_mlp_adapter' are deprecated.\n",
      " Notice: default value of tune_xxx is False, which means you would not tune this part.\n",
      "WARNING:root:Pay attention, split eval is not built...\n",
      "WARNING:root:Pay attention, split eval is not built...\n",
      "WARNING:root:Pay attention, split eval is not built...\n",
      "WARNING:root:Pay attention, split eval is not built...\n",
      "WARNING:root:Pay attention, split eval is not built...\n",
      "WARNING:root:Pay attention, split eval is not built...\n",
      "WARNING:root:Pay attention, split eval is not built...\n",
      "WARNING:root:Pay attention, split eval is not built...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mstevencai0130\u001b[0m (\u001b[33maws-llava-finetune\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ec2-user/SageMaker/efs/anker_llm_compression/scripts/VILA-catastrophic-forgetting-exp-main/wandb/run-20240912_081835-oxggosx8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mVILA1.5-3b-masked-chenwei-test\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/aws-llava-finetune/VILA\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/aws-llava-finetune/VILA/runs/oxggosx8\u001b[0m\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                         | 12/30 [06:00<07:21, 24.52s/it]"
     ]
    }
   ],
   "source": [
    "!bash scripts/v1_5/release/8b/3_sft_anker_a100.sh /home/ec2-user/SageMaker/efs/anker_llm_compression/scripts/outputs/VILA1.5-3b-normal/ VILA1.5-3b-masked-chenwei-test >> training_log.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5552bbb-0298-4308-a034-e67d9c5242e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat checkpoints/VILA1.5-3b-masked-first4-v1/llm/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f146d00-bba5-4bd0-a049-67ec67505172",
   "metadata": {},
   "source": [
    "### ËØÑÊµã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bf7c52f-cb1a-41aa-ac06-e01bb755d9a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.45s/it]\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/v1_5/eval/anker.sh checkpoints/VILA1.5-3b-masked-chenwei-v1 VILA1.5-3b-anker-mask-chenwei-v2 >> test_log.txt \n",
    "# !python scripts/v1_5/eval/analyzeResults_VHU_v6.py  --result-file ./result/VILA1.5-3b-anker-mask-chenwei-v1.txt --fixed-precision 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96615cfd-dbc7-4ad2-9fcb-151a4def9f9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 159, TN: 160, FP: 40, FN: 41, Total: 400\n",
      "Accuracy: 79.75%\n",
      "Precision: 79.90%\n",
      "Recall: 79.50%\n",
      "F1_score: 79.70%\n",
      "\n",
      "Fixed Precision: 95.00%, threshold: 0.821962833404541\n",
      "True Precision:94.64%, Recall: 53.00%, FPPI: 3.00%\n",
      "\n",
      "Fixed Recall: 95.00%, threshold: 0.1563628911972046\n",
      "True Recall: 95.00%, Precision: 62.71%, FPPI: 56.50%\n",
      "\n",
      "Fixed FPPI: 2.00%, threshold: 0.8264954090118408\n",
      "True FPPI: 2.00%, Precision: 96.30%, Recall: 52.00%\n",
      "\n",
      "FPPI: 20.00% at threshold 0.5\n",
      "FPPI: 16.00% at threshold 0.6\n",
      "FPPI: 4.00% at threshold 0.8\n",
      "FPPI: 0.00% at threshold 0.9\n",
      "FPPI: 10.50% at threshold 0.7\n",
      "FPPI: 0.00% at threshold 0.95\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python scripts/v1_5/eval/analyzeResults_VHU_v6.py  --result-file ./result/VILA1.5-3b-anker-mask-chenwei-v1.txt --fixed-precision 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "084fa5aa-0c34-4c7f-a567-612eb6425524",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 159, TN: 160, FP: 40, FN: 41, Total: 400\n",
      "Accuracy: 79.75%\n",
      "Precision: 79.90%\n",
      "Recall: 79.50%\n",
      "F1_score: 79.70%\n",
      "\n",
      "Fixed Precision: 95.00%, threshold: 0.8173599243164062\n",
      "True Precision:95.00%, Recall: 57.00%, FPPI: 3.00%\n",
      "\n",
      "Fixed Recall: 95.00%, threshold: 0.16476714611053467\n",
      "True Recall: 95.00%, Precision: 65.74%, FPPI: 49.50%\n",
      "\n",
      "Fixed FPPI: 2.00%, threshold: 0.8352636694908142\n",
      "True FPPI: 2.00%, Precision: 96.26%, Recall: 51.50%\n",
      "\n",
      "FPPI: 20.50% at threshold 0.5\n",
      "FPPI: 16.50% at threshold 0.6\n",
      "FPPI: 4.50% at threshold 0.8\n",
      "FPPI: 0.00% at threshold 0.9\n",
      "FPPI: 11.00% at threshold 0.7\n",
      "FPPI: 0.00% at threshold 0.95\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python scripts/v1_5/eval/analyzeResults_VHU_v6.py  --result-file ./result/VILA1.5-3b-anker-mask-first4-v1.txt --fixed-precision 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c1ae06-7e93-4ab9-bd19-b16e3c616fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
