{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5d3f402-4b0e-4a88-a3de-6fdb4cf0f70a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting setuptools==58.0.0\n",
      "  Downloading setuptools-58.0.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Downloading setuptools-58.0.0-py3-none-any.whl (816 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m816.3/816.3 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: setuptools\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 72.1.0\n",
      "    Uninstalling setuptools-72.1.0:\n",
      "      Successfully uninstalled setuptools-72.1.0\n",
      "Successfully installed setuptools-58.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install setuptools==58.0.0 # 降级setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2583bf44-8574-475e-8efc-5586e3afc623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!sh environment_setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd01cf06-23e8-41f2-8684-9de856c9fe1d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### 加载尝试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe234c5-f57b-4a1f-b175-cb418749bada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 剪枝后size不匹配问题：\n",
    "# 1. strict=False,不行\n",
    "# 2. 重新加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93b8f8c5-bb5c-443a-921d-56655a6b829e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d00dc38c44d4feda0f504369a282f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me something about large language models: what is happening at Google’s artificially intelligent language and context-based. How do you explain why these giant corporations are getting in on trend a little late? The company’s artificial intelligence programs are an interesting example of a technology that may be used to the benefit of the technology.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, LlamaForCausalLM\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "\n",
    "model_name = \"../outputs/vila-llm-pruned-mask\"\n",
    "\n",
    "# # 使用自定义的 Llama 模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"../outputs/vila-llm-pruned-mask\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    use_safetensors=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "# config = AutoConfig.from_pretrained(\"../outputs/VILA1.5-3b/llm\")\n",
    "# config.hidden_size = 2432  # 根据剪枝后的模型调整hidden_size\n",
    "# config.num_attention_heads = 19  # 根据剪枝后的模型调整hidden_size\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16,ignore_mismatched_sizes=True,config=config,\n",
    "#     use_safetensors=True).to(\"cuda\")\n",
    "\n",
    "# state_dict = load_file(\"../outputs/VILA1.5-3b/llm/model-00001-of-00002.safetensors\")\n",
    "# model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "# # 加载第二部分权重\n",
    "# state_dict_part2 = load_file(\"../outputs/VILA1.5-3b/llm/model-00002-of-00002.safetensors\")\n",
    "# model.load_state_dict(state_dict_part2, strict=False)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('../outputs/VILA1.5-3b/llm')\n",
    "input_text = \"Tell me something about large language models:\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids, max_length=100, num_return_sequences=1, do_sample=True)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f844e0f8-99d9-4eb6-97b3-07b75127fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要保证整除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc5d99a-09c5-4fa5-90a1-825d07779bef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, LlamaForCausalLM\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "class CustomLlamaForCausalLM(LlamaForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        print(config)\n",
    "        config.num_attention_heads = 16\n",
    "        config.head_dim = 128\n",
    "        super().__init__(config)\n",
    "        \n",
    "        for layer in self.model.layers:\n",
    "            layer.self_attn.q_proj = nn.Linear(2560, 2048)  # 自定义尺寸\n",
    "            layer.self_attn.k_proj = nn.Linear(2560, 2048)   # 自定义尺寸\n",
    "            layer.self_attn.v_proj = nn.Linear(2560, 2048)   # 自定义尺寸\n",
    "            layer.self_attn.o_proj = nn.Linear(2048, 2560)  # 自定义尺寸\n",
    "\n",
    "# # 使用自定义的 Llama 模型\n",
    "model = CustomLlamaForCausalLM.from_pretrained(\n",
    "    \"../outputs/VILA1.5-3b/llm\",\n",
    "    ignore_mismatched_sizes=True,\n",
    "    use_safetensors=True\n",
    ").to(\"cuda\")\n",
    "model.num_attention_heads = 16\n",
    "model.head_dim = 128\n",
    "\n",
    "model_name = '../outputs/VILA1.5-3b/llm'\n",
    "# config = AutoConfig.from_pretrained(\"../outputs/VILA1.5-3b/llm\")\n",
    "# config.hidden_size = 2432  # 根据剪枝后的模型调整hidden_size\n",
    "# config.num_attention_heads = 19  # 根据剪枝后的模型调整hidden_size\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16,ignore_mismatched_sizes=True,config=config,\n",
    "#     use_safetensors=True).to(\"cuda\")\n",
    "\n",
    "# state_dict = load_file(\"../outputs/VILA1.5-3b/llm/model-00001-of-00002.safetensors\")\n",
    "# model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "# # 加载第二部分权重\n",
    "# state_dict_part2 = load_file(\"../outputs/VILA1.5-3b/llm/model-00002-of-00002.safetensors\")\n",
    "# model.load_state_dict(state_dict_part2, strict=False)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "input_text = \"Tell me something about large language models:\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids, max_length=100, num_return_sequences=1, do_sample=True)\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e378a801-cf0e-41f6-bcf7-3ee9dac4576d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252169f9-b70f-46b0-8138-0ac525952fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb8ab23-89eb-4019-aaea-b3693ce41d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94a2641-c169-4786-9277-2c90887abd85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from safetensors.torch import load_file\n",
    "import torch\n",
    "\n",
    "# 1. 加载原始预训练模型\n",
    "original_model_path = \"../../../base_model/VILA1.5-3b/llm\"\n",
    "model = AutoModelForCausalLM.from_pretrained(original_model_path)\n",
    "\n",
    "# 2. 加载新的 safetensors 权重文件\n",
    "safetensor_path = \"../outputs/VILA1.5-3b/llm/model-00001-of-00002.safetensors\"\n",
    "new_weights = load_file(safetensor_path)\n",
    "\n",
    "# 3. 替换指定模块的权重（比如 self_attn.q_proj）\n",
    "with torch.no_grad():  # 避免自动梯度跟踪\n",
    "    for name, param in model.named_parameters():\n",
    "        if name in new_weights:\n",
    "            print(f\"Replacing weights for {name}\")\n",
    "            model.named_parameters()[name] = new_weights\n",
    "\n",
    "# 4. 检查替换是否成功\n",
    "for name, param in model.named_parameters():\n",
    "    if name in new_weights:\n",
    "        assert torch.allclose(param, new_weights[name]), f\"Weight replacement for {name} failed!\"\n",
    "\n",
    "# 5. 保存替换后的模型\n",
    "output_path = \"../path_to_save_new_model\"\n",
    "model.save_pretrained(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d8b022-246a-4c6f-98cb-84645fe676e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 启动训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aac2327b-9bbf-4768-abee-ac9782f8d595",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install wandb\n",
    "import wandb\n",
    "wandb.login(key='8b0b2edc05fa3d04079967e01434d12b52f5312e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "842b6756-bd6e-41df-9baa-5f41d939cd9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\t\t\t  special_tokens_map.json\n",
      "generation_config.json\t\t  tokenizer_config.json\n",
      "model-00001-of-00002.safetensors  tokenizer.model\n",
      "model-00002-of-00002.safetensors  vila-llm-pruned\n",
      "model.safetensors.index.json\n"
     ]
    }
   ],
   "source": [
    "!ls /home/ec2-user/SageMaker/efs/anker_llm_compression/scripts/outputs/VILA1.5-3b/llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e86dd6-4268-4161-918b-7023ddb999b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取当前llama代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d1ceeb01-f44f-4cca-b8b7-d64b9c926220",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py modeling_llama_origin.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcd2b68-42bf-4b1a-8dbf-24fb20b7045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用修改后的代码覆盖原有代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "03b3a89c-552e-4b06-8524-e542566b92d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp modeling_llama_modified.py /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572339ce-c0de-4f2a-bc50-3b09cf08db96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed2254c-016f-49c4-8b39-3d0679d5ec65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat /home/ec2-user/SageMaker/efs/anker_llm_compression/scripts/outputs/VILA1.5-3b-normal/llm/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2bb9d5da-cbc8-40da-8028-61e05beb663d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp config_chenwei.json ../outputs/VILA1.5-3b-normal/llm/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b3fea9-2c53-4e75-85a6-62ddea033957",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat ../outputs/VILA1.5-3b-normal/llm/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e23918-f4fc-405a-9e0a-f9d0b864d7d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scripts/v1_5/release/8b/3_sft_anker_a100.sh: line 3: scontrol: command not found\n",
      "scripts/v1_5/release/8b/3_sft_anker_a100.sh: line 6: scontrol: command not found\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "Did not find AutoResume SDK!\n",
      "Did not find AutoResume SDK!\n",
      "Did not find AutoResume SDK!\n",
      "Did not find AutoResume SDK!\n",
      "Did not find AutoResume SDK!\n",
      "Did not find AutoResume SDK!\n",
      "Did not find AutoResume SDK!\n",
      "Did not find AutoResume SDK!\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.90it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.77it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.68it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.64it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.64it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.65it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.64it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:01<00:00,  1.19it/s]\n",
      "WARNING:root:You are setting tunable parameters for the model. Previous args include 'freeze_backbone' and 'tune_mm_mlp_adapter' are deprecated.\n",
      " Notice: default value of tune_xxx is False, which means you would not tune this part.\n",
      "WARNING:root:You are setting tunable parameters for the model. Previous args include 'freeze_backbone' and 'tune_mm_mlp_adapter' are deprecated.\n",
      " Notice: default value of tune_xxx is False, which means you would not tune this part.\n",
      "WARNING:root:You are setting tunable parameters for the model. Previous args include 'freeze_backbone' and 'tune_mm_mlp_adapter' are deprecated.\n",
      " Notice: default value of tune_xxx is False, which means you would not tune this part.\n",
      "WARNING:root:You are setting tunable parameters for the model. Previous args include 'freeze_backbone' and 'tune_mm_mlp_adapter' are deprecated.\n",
      " Notice: default value of tune_xxx is False, which means you would not tune this part.\n",
      "WARNING:root:You are setting tunable parameters for the model. Previous args include 'freeze_backbone' and 'tune_mm_mlp_adapter' are deprecated.\n",
      " Notice: default value of tune_xxx is False, which means you would not tune this part.\n",
      "WARNING:root:You are setting tunable parameters for the model. Previous args include 'freeze_backbone' and 'tune_mm_mlp_adapter' are deprecated.\n",
      " Notice: default value of tune_xxx is False, which means you would not tune this part.\n",
      "WARNING:root:You are setting tunable parameters for the model. Previous args include 'freeze_backbone' and 'tune_mm_mlp_adapter' are deprecated.\n",
      " Notice: default value of tune_xxx is False, which means you would not tune this part.\n",
      "WARNING:root:You are setting tunable parameters for the model. Previous args include 'freeze_backbone' and 'tune_mm_mlp_adapter' are deprecated.\n",
      " Notice: default value of tune_xxx is False, which means you would not tune this part.\n",
      "WARNING:root:Pay attention, split eval is not built...\n",
      "WARNING:root:Pay attention, split eval is not built...\n",
      "WARNING:root:Pay attention, split eval is not built...\n",
      "WARNING:root:Pay attention, split eval is not built...\n",
      "WARNING:root:Pay attention, split eval is not built...\n",
      "WARNING:root:Pay attention, split eval is not built...\n",
      "WARNING:root:Pay attention, split eval is not built...\n",
      "WARNING:root:Pay attention, split eval is not built...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mstevencai0130\u001b[0m (\u001b[33maws-llava-finetune\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.18.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/home/ec2-user/SageMaker/efs/anker_llm_compression/scripts/VILA-catastrophic-forgetting-exp-main/wandb/run-20240912_081835-oxggosx8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mVILA1.5-3b-masked-chenwei-test\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/aws-llava-finetune/VILA\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/aws-llava-finetune/VILA/runs/oxggosx8\u001b[0m\n",
      "  0%|                                                    | 0/30 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      " 40%|█████████████████▏                         | 12/30 [06:00<07:21, 24.52s/it]"
     ]
    }
   ],
   "source": [
    "!bash scripts/v1_5/release/8b/3_sft_anker_a100.sh /home/ec2-user/SageMaker/efs/anker_llm_compression/scripts/outputs/VILA1.5-3b-normal/ VILA1.5-3b-masked-chenwei-test >> training_log.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5552bbb-0298-4308-a034-e67d9c5242e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat checkpoints/VILA1.5-3b-masked-first4-v1/llm/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f146d00-bba5-4bd0-a049-67ec67505172",
   "metadata": {},
   "source": [
    "### 评测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bf7c52f-cb1a-41aa-ac06-e01bb755d9a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:02<00:00,  1.45s/it]\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:389: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:394: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!bash scripts/v1_5/eval/anker.sh checkpoints/VILA1.5-3b-masked-chenwei-v1 VILA1.5-3b-anker-mask-chenwei-v2 >> test_log.txt \n",
    "# !python scripts/v1_5/eval/analyzeResults_VHU_v6.py  --result-file ./result/VILA1.5-3b-anker-mask-chenwei-v1.txt --fixed-precision 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96615cfd-dbc7-4ad2-9fcb-151a4def9f9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 159, TN: 160, FP: 40, FN: 41, Total: 400\n",
      "Accuracy: 79.75%\n",
      "Precision: 79.90%\n",
      "Recall: 79.50%\n",
      "F1_score: 79.70%\n",
      "\n",
      "Fixed Precision: 95.00%, threshold: 0.821962833404541\n",
      "True Precision:94.64%, Recall: 53.00%, FPPI: 3.00%\n",
      "\n",
      "Fixed Recall: 95.00%, threshold: 0.1563628911972046\n",
      "True Recall: 95.00%, Precision: 62.71%, FPPI: 56.50%\n",
      "\n",
      "Fixed FPPI: 2.00%, threshold: 0.8264954090118408\n",
      "True FPPI: 2.00%, Precision: 96.30%, Recall: 52.00%\n",
      "\n",
      "FPPI: 20.00% at threshold 0.5\n",
      "FPPI: 16.00% at threshold 0.6\n",
      "FPPI: 4.00% at threshold 0.8\n",
      "FPPI: 0.00% at threshold 0.9\n",
      "FPPI: 10.50% at threshold 0.7\n",
      "FPPI: 0.00% at threshold 0.95\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python scripts/v1_5/eval/analyzeResults_VHU_v6.py  --result-file ./result/VILA1.5-3b-anker-mask-chenwei-v1.txt --fixed-precision 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "084fa5aa-0c34-4c7f-a567-612eb6425524",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 159, TN: 160, FP: 40, FN: 41, Total: 400\n",
      "Accuracy: 79.75%\n",
      "Precision: 79.90%\n",
      "Recall: 79.50%\n",
      "F1_score: 79.70%\n",
      "\n",
      "Fixed Precision: 95.00%, threshold: 0.8173599243164062\n",
      "True Precision:95.00%, Recall: 57.00%, FPPI: 3.00%\n",
      "\n",
      "Fixed Recall: 95.00%, threshold: 0.16476714611053467\n",
      "True Recall: 95.00%, Precision: 65.74%, FPPI: 49.50%\n",
      "\n",
      "Fixed FPPI: 2.00%, threshold: 0.8352636694908142\n",
      "True FPPI: 2.00%, Precision: 96.26%, Recall: 51.50%\n",
      "\n",
      "FPPI: 20.50% at threshold 0.5\n",
      "FPPI: 16.50% at threshold 0.6\n",
      "FPPI: 4.50% at threshold 0.8\n",
      "FPPI: 0.00% at threshold 0.9\n",
      "FPPI: 11.00% at threshold 0.7\n",
      "FPPI: 0.00% at threshold 0.95\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python scripts/v1_5/eval/analyzeResults_VHU_v6.py  --result-file ./result/VILA1.5-3b-anker-mask-first4-v1.txt --fixed-precision 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c1ae06-7e93-4ab9-bd19-b16e3c616fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
