{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f588da44-b67e-4f2c-8796-b09324e15489",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a913c0f6-445a-4f13-abcb-676ebe3a00fb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 普通模型删减head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e0025a2-d0d6-4774-ab53-cb9515393563",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_attention_layer:MultiheadAttention(\n",
      "  (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      ")\n",
      "剪枝后的多头注意力层已保存！\n"
     ]
    }
   ],
   "source": [
    "!python3 example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a60784-5d55-4e74-9fb9-bcf032919d57",
   "metadata": {
    "tags": []
   },
   "source": [
    "### vila模型删减head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "004f7631-834c-40b5-a4bc-860ed7735898",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json  llm  mm_projector\tREADME.md  trainer_state.json  vision_tower\n"
     ]
    }
   ],
   "source": [
    "!ls ../../base_model/VILA1.5-3b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb981eb6-0690-469a-8b41-7f6ef93e4866",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### 观察模型，构造删减head json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f9aae6-bc8c-47b6-be6b-4d864a08bee1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoModel\n",
    "\n",
    "# Load the VILA 1.5-3B model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"../../base_model/VILA1.5-3b/llm\"\n",
    "model = AutoModel.from_pretrained(model_name, torch_dtype=torch.bfloat16).to(device)\n",
    "\n",
    "keys = []\n",
    "for name, module in model.named_modules():\n",
    "    print(name, type(module))\n",
    "\n",
    "    # LLama里是transformers.models.llama.modeling_llama.LlamaSdpaAttention，不是nn.MultiheadAttention\n",
    "    # if isinstance(module, nn.MultiheadAttention):\n",
    "    if 'self_attn' in name:\n",
    "        if name.split('.')[-1]=='self_attn':\n",
    "            keys.append(\"model.\"+name) # 注意这里命名有区别\n",
    "            print(f\"{name} module.num_heads: {module.num_heads}\")\n",
    "            \n",
    "print(keys)\n",
    "content = {}\n",
    "for k in keys:\n",
    "    # content[f'model.layers.{i}.self_attn'] = [0,1,2]\n",
    "    content[k] = [0,1,2,3]\n",
    "\n",
    "with open(\"prune_dict_vila.json\",'w') as output_file:\n",
    "    output_file.write(json.dumps(content, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982a22a0-dd1c-4da4-9a3f-3c30029e4c37",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 删减head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eb5d49-7362-49d5-a21f-d92c08451cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 example_vila.py --pretrained_model '../../base_model/VILA1.5-3b/llm' --prune_dict 'prune_dict_vila.json' --output_path 'outputs/vila-llm-pruned-mask'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3c2c2e1-624a-45a7-8f8b-e31d2f80251e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00,  3.15it/s]\n",
      "layer model.layers.0.self_attn has 20 heads\n",
      "Pruning heads [0, 1, 2, 3] in layer model.layers.0.self_attn\n",
      "layer model.layers.1.self_attn has 20 heads\n",
      "Pruning heads [0, 1, 2, 3] in layer model.layers.1.self_attn\n",
      "layer model.layers.2.self_attn has 20 heads\n",
      "Pruning heads [0, 1, 2, 3] in layer model.layers.2.self_attn\n",
      "layer model.layers.3.self_attn has 20 heads\n",
      "Pruning heads [0, 1, 2, 3] in layer model.layers.3.self_attn\n",
      "layer model.layers.4.self_attn has 20 heads\n",
      "Pruning heads [0, 1, 2, 3] in layer model.layers.4.self_attn\n",
      "layer model.layers.5.self_attn has 20 heads\n",
      "Pruning heads [0, 1, 2, 3] in layer model.layers.5.self_attn\n",
      "layer model.layers.6.self_attn has 20 heads\n",
      "Pruning heads [0, 1, 2, 3] in layer model.layers.6.self_attn\n",
      "layer model.layers.7.self_attn has 20 heads\n",
      "Pruning heads [0, 1, 2, 3] in layer model.layers.7.self_attn\n",
      "layer model.layers.8.self_attn has 20 heads\n",
      "Pruning heads [0, 1, 2, 3] in layer model.layers.8.self_attn\n",
      "layer model.layers.9.self_attn has 20 heads\n",
      "Pruning heads [0, 1, 2, 3] in layer model.layers.9.self_attn\n",
      "layer model.layers.10.self_attn has 20 heads\n",
      "Pruning heads [0, 1, 2, 3] in layer model.layers.10.self_attn\n",
      "layer model.layers.11.self_attn has 20 heads\n",
      "Pruning heads [0, 1, 2, 3] in layer model.layers.11.self_attn\n",
      "layer model.layers.12.self_attn has 20 heads\n",
      "Pruning heads [0, 1, 2, 3] in layer model.layers.12.self_attn\n",
      "layer model.layers.13.self_attn has 20 heads\n",
      "Pruning heads [0, 1, 2, 3] in layer model.layers.13.self_attn\n",
      "layer model.layers.14.self_attn has 20 heads\n",
      "Pruning heads [0, 1, 2, 3] in layer model.layers.14.self_attn\n",
      "layer model.layers.15.self_attn has 20 heads\n",
      "Pruning heads [0, 1, 2, 3] in layer model.layers.15.self_attn\n",
      "layer model.layers.16.self_attn has 20 heads\n",
      "Pruning heads [0, 1, 2, 3] in layer model.layers.16.self_attn\n",
      "layer model.layers.17.self_attn has 20 heads\n",
      "Pruning heads [0, 1, 2, 3] in layer model.layers.17.self_attn\n",
      "layer model.layers.18.self_attn has 20 heads\n",
      "Pruning heads [0, 1, 2, 3] in layer model.layers.18.self_attn\n",
      "layer model.layers.19.self_attn has 20 heads\n",
      "Pruning heads [0, 1, 2, 3] in layer model.layers.19.self_attn\n",
      "layer model.layers.20.self_attn has 20 heads\n",
      "Pruning heads [0, 1, 2, 3] in layer model.layers.20.self_attn\n",
      "layer model.layers.21.self_attn has 20 heads\n",
      "Pruning heads [0, 1, 2, 3] in layer model.layers.21.self_attn\n",
      "layer model.layers.22.self_attn has 20 heads\n",
      "Pruning heads [0, 1, 2, 3] in layer model.layers.22.self_attn\n",
      "layer model.layers.23.self_attn has 20 heads\n",
      "Pruning heads [0, 1, 2, 3] in layer model.layers.23.self_attn\n",
      "layer model.layers.24.self_attn has 20 heads\n",
      "Pruning heads [0, 1, 2, 3] in layer model.layers.24.self_attn\n",
      "layer model.layers.25.self_attn has 20 heads\n",
      "Pruning heads [0, 1, 2, 3] in layer model.layers.25.self_attn\n",
      "layer model.layers.26.self_attn has 20 heads\n",
      "Pruning heads [0, 1, 2, 3] in layer model.layers.26.self_attn\n",
      "layer model.layers.27.self_attn has 20 heads\n",
      "Pruning heads [0, 1, 2, 3] in layer model.layers.27.self_attn\n",
      "layer model.layers.28.self_attn has 20 heads\n",
      "Pruning heads [0, 1, 2, 3] in layer model.layers.28.self_attn\n",
      "layer model.layers.29.self_attn has 20 heads\n",
      "Pruning heads [0, 1, 2, 3] in layer model.layers.29.self_attn\n",
      "layer model.layers.30.self_attn has 20 heads\n",
      "Pruning heads [0, 1, 2, 3] in layer model.layers.30.self_attn\n",
      "layer model.layers.31.self_attn has 20 heads\n",
      "Pruning heads [0, 1, 2, 3] in layer model.layers.31.self_attn\n",
      "Generated Text:\n",
      "Tell me something about large language models: They do not have to give the best testimonials. A very good news to get started on. Some time, one of the people with the large language models get the good news but what kind of training to get more information in the world!\n"
     ]
    }
   ],
   "source": [
    "!python3 example_vila_mask.py --pretrained_model '../../base_model/VILA1.5-3b/llm' --prune_dict 'prune_dict_vila.json' --output_path 'outputs/vila-llm-pruned-mask'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9466077a-9798-4765-9dec-70b246d53d14",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### llama3-8B模型删减head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd60ffbb-34f1-4a15-9045-f60301e6c760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用llama3模型做实验，假设删减掉每个attention层的前3个head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a724be3-edf4-4f08-a0ca-caa8dbc12a2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5ea2f5cbd748c5a8f7725c4f11f897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn module.num_heads: 32\n",
      "model.layers.1.self_attn module.num_heads: 32\n",
      "model.layers.2.self_attn module.num_heads: 32\n",
      "model.layers.3.self_attn module.num_heads: 32\n",
      "model.layers.4.self_attn module.num_heads: 32\n",
      "model.layers.5.self_attn module.num_heads: 32\n",
      "model.layers.6.self_attn module.num_heads: 32\n",
      "model.layers.7.self_attn module.num_heads: 32\n",
      "model.layers.8.self_attn module.num_heads: 32\n",
      "model.layers.9.self_attn module.num_heads: 32\n",
      "model.layers.10.self_attn module.num_heads: 32\n",
      "model.layers.11.self_attn module.num_heads: 32\n",
      "model.layers.12.self_attn module.num_heads: 32\n",
      "model.layers.13.self_attn module.num_heads: 32\n",
      "model.layers.14.self_attn module.num_heads: 32\n",
      "model.layers.15.self_attn module.num_heads: 32\n",
      "model.layers.16.self_attn module.num_heads: 32\n",
      "model.layers.17.self_attn module.num_heads: 32\n",
      "model.layers.18.self_attn module.num_heads: 32\n",
      "model.layers.19.self_attn module.num_heads: 32\n",
      "model.layers.20.self_attn module.num_heads: 32\n",
      "model.layers.21.self_attn module.num_heads: 32\n",
      "model.layers.22.self_attn module.num_heads: 32\n",
      "model.layers.23.self_attn module.num_heads: 32\n",
      "model.layers.24.self_attn module.num_heads: 32\n",
      "model.layers.25.self_attn module.num_heads: 32\n",
      "model.layers.26.self_attn module.num_heads: 32\n",
      "model.layers.27.self_attn module.num_heads: 32\n",
      "model.layers.28.self_attn module.num_heads: 32\n",
      "model.layers.29.self_attn module.num_heads: 32\n",
      "model.layers.30.self_attn module.num_heads: 32\n",
      "model.layers.31.self_attn module.num_heads: 32\n",
      "['model.layers.0.self_attn', 'model.layers.1.self_attn', 'model.layers.2.self_attn', 'model.layers.3.self_attn', 'model.layers.4.self_attn', 'model.layers.5.self_attn', 'model.layers.6.self_attn', 'model.layers.7.self_attn', 'model.layers.8.self_attn', 'model.layers.9.self_attn', 'model.layers.10.self_attn', 'model.layers.11.self_attn', 'model.layers.12.self_attn', 'model.layers.13.self_attn', 'model.layers.14.self_attn', 'model.layers.15.self_attn', 'model.layers.16.self_attn', 'model.layers.17.self_attn', 'model.layers.18.self_attn', 'model.layers.19.self_attn', 'model.layers.20.self_attn', 'model.layers.21.self_attn', 'model.layers.22.self_attn', 'model.layers.23.self_attn', 'model.layers.24.self_attn', 'model.layers.25.self_attn', 'model.layers.26.self_attn', 'model.layers.27.self_attn', 'model.layers.28.self_attn', 'model.layers.29.self_attn', 'model.layers.30.self_attn', 'model.layers.31.self_attn']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"../../base_model/Meta-Llama-3-8B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).to(device)\n",
    "\n",
    "keys = []\n",
    "for name, module in model.named_modules():\n",
    "    # print(name, type(module))\n",
    "\n",
    "    # LLama里是transformers.models.llama.modeling_llama.LlamaSdpaAttention，不是nn.MultiheadAttention\n",
    "    # if isinstance(module, nn.MultiheadAttention):\n",
    "    if 'self_attn' in name:\n",
    "        if name.split('.')[-1]=='self_attn':\n",
    "            keys.append(name)\n",
    "            print(f\"{name} module.num_heads: {module.num_heads}\")\n",
    "            \n",
    "print(keys)\n",
    "content = {}\n",
    "for k in keys:\n",
    "    # content[f'model.layers.{i}.self_attn'] = [0,1,2]\n",
    "    content[k] = [0,1,2]\n",
    "\n",
    "with open(\"prune_dict_llama3.json\",'w') as output_file:\n",
    "    output_file.write(json.dumps(content, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f9f5e8-f0aa-4b74-a5af-e6757accc3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama3-8B model比较特殊，https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py\n",
    "# 它使用了group attention，以至于它的query num_heads 和k,v num_heads 是不一样多的\n",
    "# self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n",
    "# self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads\n",
    "# 这里的逻辑需要单独处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88d35a87-7711-4320-a8c5-7be96cb127dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:00<00:00, 13.73it/s]\n",
      "layer model.layers.0.self_attn has 32 heads\n",
      "Pruning heads [0, 1, 2] in layer model.layers.0.self_attn\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "layer model.layers.0.self_attn remains 29 heads\n",
      "layer model.layers.1.self_attn has 32 heads\n",
      "Pruning heads [0, 1, 2] in layer model.layers.1.self_attn\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "layer model.layers.1.self_attn remains 29 heads\n",
      "layer model.layers.2.self_attn has 32 heads\n",
      "Pruning heads [0, 1, 2] in layer model.layers.2.self_attn\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "layer model.layers.2.self_attn remains 29 heads\n",
      "layer model.layers.3.self_attn has 32 heads\n",
      "Pruning heads [0, 1, 2] in layer model.layers.3.self_attn\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "layer model.layers.3.self_attn remains 29 heads\n",
      "layer model.layers.4.self_attn has 32 heads\n",
      "Pruning heads [0, 1, 2] in layer model.layers.4.self_attn\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "layer model.layers.4.self_attn remains 29 heads\n",
      "layer model.layers.5.self_attn has 32 heads\n",
      "Pruning heads [0, 1, 2] in layer model.layers.5.self_attn\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "layer model.layers.5.self_attn remains 29 heads\n",
      "layer model.layers.6.self_attn has 32 heads\n",
      "Pruning heads [0, 1, 2] in layer model.layers.6.self_attn\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "layer model.layers.6.self_attn remains 29 heads\n",
      "layer model.layers.7.self_attn has 32 heads\n",
      "Pruning heads [0, 1, 2] in layer model.layers.7.self_attn\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "layer model.layers.7.self_attn remains 29 heads\n",
      "layer model.layers.8.self_attn has 32 heads\n",
      "Pruning heads [0, 1, 2] in layer model.layers.8.self_attn\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "layer model.layers.8.self_attn remains 29 heads\n",
      "layer model.layers.9.self_attn has 32 heads\n",
      "Pruning heads [0, 1, 2] in layer model.layers.9.self_attn\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "layer model.layers.9.self_attn remains 29 heads\n",
      "layer model.layers.10.self_attn has 32 heads\n",
      "Pruning heads [0, 1, 2] in layer model.layers.10.self_attn\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "layer model.layers.10.self_attn remains 29 heads\n",
      "layer model.layers.11.self_attn has 32 heads\n",
      "Pruning heads [0, 1, 2] in layer model.layers.11.self_attn\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "layer model.layers.11.self_attn remains 29 heads\n",
      "layer model.layers.12.self_attn has 32 heads\n",
      "Pruning heads [0, 1, 2] in layer model.layers.12.self_attn\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "layer model.layers.12.self_attn remains 29 heads\n",
      "layer model.layers.13.self_attn has 32 heads\n",
      "Pruning heads [0, 1, 2] in layer model.layers.13.self_attn\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "layer model.layers.13.self_attn remains 29 heads\n",
      "layer model.layers.14.self_attn has 32 heads\n",
      "Pruning heads [0, 1, 2] in layer model.layers.14.self_attn\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "layer model.layers.14.self_attn remains 29 heads\n",
      "layer model.layers.15.self_attn has 32 heads\n",
      "Pruning heads [0, 1, 2] in layer model.layers.15.self_attn\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "layer model.layers.15.self_attn remains 29 heads\n",
      "layer model.layers.16.self_attn has 32 heads\n",
      "Pruning heads [0, 1, 2] in layer model.layers.16.self_attn\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "layer model.layers.16.self_attn remains 29 heads\n",
      "layer model.layers.17.self_attn has 32 heads\n",
      "Pruning heads [0, 1, 2] in layer model.layers.17.self_attn\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "layer model.layers.17.self_attn remains 29 heads\n",
      "layer model.layers.18.self_attn has 32 heads\n",
      "Pruning heads [0, 1, 2] in layer model.layers.18.self_attn\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "layer model.layers.18.self_attn remains 29 heads\n",
      "layer model.layers.19.self_attn has 32 heads\n",
      "Pruning heads [0, 1, 2] in layer model.layers.19.self_attn\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "layer model.layers.19.self_attn remains 29 heads\n",
      "layer model.layers.20.self_attn has 32 heads\n",
      "Pruning heads [0, 1, 2] in layer model.layers.20.self_attn\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "layer model.layers.20.self_attn remains 29 heads\n",
      "layer model.layers.21.self_attn has 32 heads\n",
      "Pruning heads [0, 1, 2] in layer model.layers.21.self_attn\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "layer model.layers.21.self_attn remains 29 heads\n",
      "layer model.layers.22.self_attn has 32 heads\n",
      "Pruning heads [0, 1, 2] in layer model.layers.22.self_attn\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "layer model.layers.22.self_attn remains 29 heads\n",
      "layer model.layers.23.self_attn has 32 heads\n",
      "Pruning heads [0, 1, 2] in layer model.layers.23.self_attn\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "layer model.layers.23.self_attn remains 29 heads\n",
      "layer model.layers.24.self_attn has 32 heads\n",
      "Pruning heads [0, 1, 2] in layer model.layers.24.self_attn\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "layer model.layers.24.self_attn remains 29 heads\n",
      "layer model.layers.25.self_attn has 32 heads\n",
      "Pruning heads [0, 1, 2] in layer model.layers.25.self_attn\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "layer model.layers.25.self_attn remains 29 heads\n",
      "layer model.layers.26.self_attn has 32 heads\n",
      "Pruning heads [0, 1, 2] in layer model.layers.26.self_attn\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "layer model.layers.26.self_attn remains 29 heads\n",
      "layer model.layers.27.self_attn has 32 heads\n",
      "Pruning heads [0, 1, 2] in layer model.layers.27.self_attn\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "layer model.layers.27.self_attn remains 29 heads\n",
      "layer model.layers.28.self_attn has 32 heads\n",
      "Pruning heads [0, 1, 2] in layer model.layers.28.self_attn\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "layer model.layers.28.self_attn remains 29 heads\n",
      "layer model.layers.29.self_attn has 32 heads\n",
      "Pruning heads [0, 1, 2] in layer model.layers.29.self_attn\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "layer model.layers.29.self_attn remains 29 heads\n",
      "layer model.layers.30.self_attn has 32 heads\n",
      "Pruning heads [0, 1, 2] in layer model.layers.30.self_attn\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "layer model.layers.30.self_attn remains 29 heads\n",
      "layer model.layers.31.self_attn has 32 heads\n",
      "Pruning heads [0, 1, 2] in layer model.layers.31.self_attn\n",
      "LlamaSdpaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "layer model.layers.31.self_attn remains 29 heads\n",
      "pruning done\n",
      "[2024-09-09 06:56:20,121] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n"
     ]
    }
   ],
   "source": [
    "!python3 example_llama.py --pretrained_model '../../base_model/Meta-Llama-3-8B-Instruct' --prune_dict 'prune_dict_llama3.json' --output_path 'outputs/llama3-8b-instruct-pruned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3def16a3-8baf-4c08-b592-4140aac3aa27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d215acfa63a74d2cbf10879b0e3c6fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn module.num_heads: 32\n",
      "model.layers.1.self_attn module.num_heads: 32\n",
      "model.layers.2.self_attn module.num_heads: 32\n",
      "model.layers.3.self_attn module.num_heads: 32\n",
      "model.layers.4.self_attn module.num_heads: 32\n",
      "model.layers.5.self_attn module.num_heads: 32\n",
      "model.layers.6.self_attn module.num_heads: 32\n",
      "model.layers.7.self_attn module.num_heads: 32\n",
      "model.layers.8.self_attn module.num_heads: 32\n",
      "model.layers.9.self_attn module.num_heads: 32\n",
      "model.layers.10.self_attn module.num_heads: 32\n",
      "model.layers.11.self_attn module.num_heads: 32\n",
      "model.layers.12.self_attn module.num_heads: 32\n",
      "model.layers.13.self_attn module.num_heads: 32\n",
      "model.layers.14.self_attn module.num_heads: 32\n",
      "model.layers.15.self_attn module.num_heads: 32\n",
      "model.layers.16.self_attn module.num_heads: 32\n",
      "model.layers.17.self_attn module.num_heads: 32\n",
      "model.layers.18.self_attn module.num_heads: 32\n",
      "model.layers.19.self_attn module.num_heads: 32\n",
      "model.layers.20.self_attn module.num_heads: 32\n",
      "model.layers.21.self_attn module.num_heads: 32\n",
      "model.layers.22.self_attn module.num_heads: 32\n",
      "model.layers.23.self_attn module.num_heads: 32\n",
      "model.layers.24.self_attn module.num_heads: 32\n",
      "model.layers.25.self_attn module.num_heads: 32\n",
      "model.layers.26.self_attn module.num_heads: 32\n",
      "model.layers.27.self_attn module.num_heads: 32\n",
      "model.layers.28.self_attn module.num_heads: 32\n",
      "model.layers.29.self_attn module.num_heads: 32\n",
      "model.layers.30.self_attn module.num_heads: 32\n",
      "model.layers.31.self_attn module.num_heads: 32\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"outputs/llama3-8b-instruct-pruned\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).to(device)\n",
    "\n",
    "keys = []\n",
    "for name, module in model.named_modules():\n",
    "    # print(name, type(module))\n",
    "\n",
    "    # LLama里是transformers.models.llama.modeling_llama.LlamaSdpaAttention，不是nn.MultiheadAttention\n",
    "    # if isinstance(module, nn.MultiheadAttention):\n",
    "    if 'self_attn' in name:\n",
    "        if name.split('.')[-1]=='self_attn':\n",
    "            keys.append(name)\n",
    "            print(f\"{name} module.num_heads: {module.num_heads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d1e4a1-8c8d-46f9-85ef-be426e8ac845",
   "metadata": {},
   "source": [
    "### 训练删减后的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0b384199-2ae9-4d1d-ae54-a91a35065834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r ../../base_model/VILA1.5-3b outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f34f51c-8ad0-45c4-aaff-819becf1a6b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r outputs/vila-llm-pruned-mask outputs/VILA1.5-3b/llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0756aa-73b5-4bc3-ac20-9378bef3c767",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!bash VILA-catastrophic-forgetting-exp-main/scripts/v1_5/release/8b/3_sft_anker.sh outputs/VILA1.5-3b outputs/VILA1.5-3b-trained"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
